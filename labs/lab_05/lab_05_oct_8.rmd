---
title: "lab_05"
author: "Derek Willis"
date: "2025-01-16"
output: html_document
---
student name: Raphael Romero Ruiz
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

-   Tabula

## Load tidyverse, janitor and lubidate libraries and establish settings


```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse, plus any other packages you will need to clean data and work with dates.
library(tidyverse)
library(janitor)
library(lubridate)
```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore
County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link).

Examine the document:

-   DAT_FORMAT is date of the incident

-   DAT_FORMACAATSE_NBR has the date and time of the incident

-   CASE_NBR is the case number

-   EVTYP is event_type

-   LOC is the location

Download it to your labs folder. You will extract the tables within it,
export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Launch Tabula (you may then need to go to <http://127.0.0.1:8080/> in
your browser). Click the "Browse" button and find the PDF file and click
"open", and then click the "Import button" in Tabula.

This PDF has a single table spread over multiple pages to extract. We're
going to make a single dataframe from this table, exporting it to a CSV
file that you will load into R. In Tabula, highlight the table and click
the "Preview & Export Extracted Data" button.

**HINT: You may want to play with including or excluding the column
headers.**

**YOU MUST HAVE FIVE COLUMNS OF DATA FOR EXPORT.**

The final entry should be 02/06/23 at 5651 BRAXFIELD RD

Save the CSV (it should be called
`tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to
your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers.

You can choose to include the headers from the PDF in your exported
CSV files OR to exclude them and add them when importing. 
`read_csv` allows us to do this ([and
more](https://readr.tidyverse.org/reference/read_delim.html)).
Note the col_names entry: col_names
Either TRUE, FALSE or a character vector of column names.
If TRUE, the first row of the input will be used as the column names, and will not be included in the data frame. If FALSE, column names will be generated automatically: X1, X2, X3 etc.

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used
for analysis. By "clean" I mean the column headers should not contain
spaces and they should have meaningful names, not "x1" or something
similar. How you do that is up to you, but you can use select() with or
without the minus sign to include or exclude certain columns. You also
can use the `rename` function to, well, rename columns. Importantly,
you'll need to ensure that any columns containing a date actually have a
date datatype. `lubridate` can help with this.

```{r}
overdose_calls_baltimore_county <- read_csv("labs/lab_05/data/tabula-Baltimore County; Carey, Samantha log OD.csv", col_names = FALSE) %>%
  clean_names() %>%
  rename(
    date_format = x1,
    time_format = x2,
    case_number = x3,
    ev_type = x4,
    location = x5
  ) %>% 
  mutate(date_format = mdy(date_format))
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each
date. 

A1.

```{r}
overdose_calls_baltimore_county %>% 
  group_by(date_format) %>% 
  summarise(count=n()) %>% 
  arrange(desc(date_format))
```

Q1a: Write down this answer: Which date in 2022 had the most overdose calls, and how many?
**Q1a answer**:
There is a tie between July 14, 2022 and October 4, 2022 with 23 registered 911 overdose calls on both dates. 

```{r}
overdose_calls_baltimore_county %>% 
  filter(year(date_format) == 2022) %>%
  group_by(date_format) %>%
  summarise(count = n()) %>%
  slice_max(count, n = 1)
```


Q1b: Write down this answer: Look at the total number of rows in your result from Q1. How many dates are represented in this data? **Q1b answer**:
There is a year's worth of data in this data frame from Feb. 6, 2023 to Feb. 6, 2022 - though there are only 366 days. 

Q2. You want to see if overdose calls fall on a particular day of the week.

**Write code for the following:**
Add a column to your dataframe that shows the day of the week represented by each date. 

You can do this using lubridate. Look up how you would do this. https://lubridate.tidyverse.org/

Then write code to calculate:
1) the number of calls for each day of the week
2) and add a column that calculates the percentage of all calls on each
day of the week.

Your result will have the day of the week, total
number of calls and the percentage of calls on that day out of the total
number of all calls. 


```{r}
overdose_calls_baltimore_county <- overdose_calls_baltimore_county %>%
  mutate(day_of_week=wday(date_format, label=TRUE))

wday_od_calls_balcounty <- overdose_calls_baltimore_county %>%
  group_by(day_of_week) %>% 
  summarise(total_calls = n()) %>%
  mutate(percentage = (total_calls / sum(total_calls)) * 100)
```


**Q2a. Write an answer describing your findings.**

A2a. The calls happen more frequently on the weekend, particularly Saturdays and Sundays, compared to weekdays. This pattern suggests that drug use and overdose incidents may be more prevalent during these days. However, it's important to note that the number of calls on each day is relatively low, with the highest being around 10-15 calls per day.

Q3. Now let's look at locations. 

**Write code for the following:**
Which locations have the most calls?

A3.

```{r}
overdose_calls_baltimore_county %>%
  group_by(location) %>%
  summarise(total_calls = n()) %>%
  arrange(desc(total_calls))
```

**Q3a. Write an answer to the following:**
How would you describe them? Search on the Web for more information. Is there anything about the structure of the original data that might make you less confident in the counts by location or date?
The locations with the most overdose calls appear to be specific residential addresses, with several addresses showing unusually high concentrations of calls. When researching these top locations online, I would look for information like: Are these addresses known problem areas or drug activity hotspots?; Are they near treatment facilities, homeless shelters, or social services?; What is the socioeconomic conditions in the neighborhood where the incident has occurred?
With this is in mind, I will say I feel less confident in these location counts due to several data structure issues. There are some address inconsistencies and potential duplicates that could skew the results. For example: "123 MAIN ST" vs "123 Main Street" vs "123 MAIN STREET". This would create a split for what should be counted as one location. Some of the addresses also have partial information, which could lead to undercounting at specific addresses. Additionally, if there are multiple 911 calls for the same overdose incident that would inflate counts at certain locations. I believe that these issues make it difficult to determine if the high-count locations represent genuine hotspots or due to inconsistent data recording.

**Q4.  Write an answer to the following:** What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. I would pursue a story about the repeat overdose 911 calls in certain residential addresses, based on what we see thus far in this short analysis. Of course I think the analysis is part of the story, a key driver really, to produce something that could better contextualize what is often a sensationalized topic. The Baltimore Banner has produced these kinds of stories with great success as of late, which super important for local newsroom that exists in a community that has been ravaged by the opiod epidemic. 
